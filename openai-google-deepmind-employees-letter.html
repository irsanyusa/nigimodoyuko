<!doctype html><html lang=en><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="A group of current and former employees at leading AI companies OpenAI and Google DeepMind published a letter on Tuesday warning against the dangers of advanced AI as they allege companies are prioritizing financial gains while avoiding oversight."><meta name=robots content="index,follow,noarchive"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><title>Employees Say OpenAI and Google DeepMind Are Hiding Dangers</title><link rel=canonical href=./openai-google-deepmind-employees-letter.html><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a href=./index.html>ZBlogH</a></h1><ul><li><a href=./index.xml>Rss</a></li><li><a href=./sitemap.xml>Sitemap</a></li></ul></section><section id=content><h1>Employees Say OpenAI and Google DeepMind Are Hiding Dangers</h1><div id=sub-header>August 2024 · 4 minute read</div><div class=entry-content><img src=https://cdn.statically.io/img/api.time.com/wp-content/uploads/2024/06/GettyImages-2155035826.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">A group of current and former employees at leading AI companies OpenAI and Google DeepMind published a letter on Tuesday warning against the dangers of advanced AI as they allege<strong> </strong>companies are prioritizing financial gains while avoiding oversight.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Thirteen employees, eleven of which are current or former employees of <a href=#>OpenAI</a>, the company behind ChatGPT, signed <a href=#>the letter</a> entitled: “A Right to Warn about Advanced Artificial Intelligence.” The<strong> </strong>two<strong> </strong>other signatories<strong> </strong>are current and former employees of Google DeepMind. Six individuals are anonymous.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The coalition cautions that AI systems are powerful enough to pose serious harms without proper regulation. “These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction,” the letter says.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More: </strong><a href=#>Exclusive: U.S. Must Move ‘Decisively’ to Avert ‘Extinction-Level’ Threat From AI, Government-Commissioned Report Says</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">“We’re proud of our track record providing the most capable and safest A.I. systems and believe in our scientific approach to addressing risk,” OpenAI spokeswoman Lindsey Held <a href=#>told the New York Times</a>. “We agree that rigorous debate is crucial given the significance of this technology, and we’ll continue to engage with governments, civil society and other communities around the world.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Google DeepMind has not commented publicly on the letter and did not respond to TIME’s request for comment.&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Leaders of all three leading AI companies—OpenAI, Google DeepMind and Anthropic—have talked about the risks in the past.<strong> </strong>“If we build an AI system that’s significantly more competent than human experts but it pursues goals that conflict with our best interests, the consequences could be dire… rapid AI progress would be very disruptive, changing employment, macroeconomics, and power structures … [we have already encountered] toxicity, bias, unreliability, dishonesty,” AI safety and research company Anthropic <a href=#>said</a> in a March 2023 statement, which is linked to in the letter. (One of the letter signatories who currently works at Google DeepMind used to work at Anthropic.)</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More: </strong><a href=#>Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The group behind the letter alleges that AI companies have information about the risks of the AI technology they are working on, but because they aren’t required<strong> </strong>to disclose much with governments, the real capabilities of their systems remain a secret. That means current and former employees are the only ones who can hold the companies accountable to the public, they say, and yet many have found their hands tied by confidentiality agreements that prevent workers from voicing their concerns publicly. “Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated,” the group wrote.&nbsp;&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">“Employees are an important line of safety defense, and if they can’t speak freely without retribution, that channel’s going to be shut down,” the group’s pro bono lawyer Lawrence Lessig told the New York Times.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Eighty-three percent of Americans believe that AI could accidentally lead to a catastrophic event, according to <a href=#>research by the AI Policy Institute</a>. Another 82% <a href=#>do not trust tech executives</a> to self-regulate the industry. Daniel Colson, executive director of the Institute, notes that the letter has come out after a series of high-profile exits from OpenAI, including Chief Scientist Ilya Sutskever. Sutskever’s departure also made public the non-disparagement agreements that former employees would sign to bar them from speaking negatively about the company. Failure to abide by that rule would put their vested equity at risk. “There needs to be an ability for employees and whistleblowers to share what's going on and share their concerns,” says Colson. “Things that restrict the people in the know from speaking about what's actually happening really undermines the ability for us to make good choices about how to develop technology.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The letter writers have made four demands of advanced AI companies: stop forcing employees into agreements that prevent them from criticizing their employer for “risk-related concerns,” create an anonymous process for employees to raise their concerns to board members and other relevant regulators or organizations, support a “culture of open criticism,” and not retaliate against former and current employees who share “risk-related confidential information after other processes have failed.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">“The thing that we haven't seen at all anywhere,” Colson says, “is requirements being placed upon these companies for things like safety testing, and any sort of limitation on companies being able to develop these models if they don't comply with cybersecurity requirements, or safety testing requirements.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Governments around the world have moved to regulate AI, though progress lags behind the <a href=#>speed at which AI is progressing</a>. Earlier this year, the E.U. passed the world’s first comprehensive AI legislation. Efforts at international cooperation have been pursued through <a href=#>AI Safety Summits</a> in the U.K. and South Korea, and <a href=#>at the U.N</a>. In October 2023. President Joe Biden signed an <a href=#>AI executive order</a> that, among other things, requires AI companies to disclose their development and safety testing plans to the Department of Commerce.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">-With additional reporting by Will Henshall/Washington</p><h3 class="text-[1.17rem] font-bold tracking-0.5px font-zilla-slab self-baseline">More From TIME</h3><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmismaKyb6%2FOpmZvcWhqgnGAjqinnqaRnnqou86go55llJqysbnIp5tmnZ2lubDFxJ6qZqSVqcGmvo4%3D</p></div><div id=links><a href=./relationship-tips-making-a-sex-tape-with-your-partner-is-worth-considering.html>&#171;&nbsp;Making a sex tape with your partner is worth considering</a>
<a href=./5-interior-features-of-a-japanese-house-that-make-it-the-most-convenient-place-to-live-443210.html>5 Interior Features of a Japanese House That Make It the Most Convenient Place to Live&nbsp;&#187;</a></div></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>